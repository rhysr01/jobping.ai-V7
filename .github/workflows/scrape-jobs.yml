name: Automated Job Scraping

on:
  schedule:
    # Run every 4 hours
    - cron: '0 */4 * * *'
  
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      platforms:
        description: 'Platforms to scrape (comma-separated)'
        required: false
        default: 'all'
        type: string
      cleanup_only:
        description: 'Run cleanup only'
        required: false
        default: false
        type: boolean

concurrency:
  group: scrape
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Run scraping
      env:
        SCRAPE_API_URL: ${{ secrets.SCRAPE_API_URL }}
        CLEANUP_API_URL: ${{ secrets.CLEANUP_API_URL }}
        SCRAPE_API_KEY: ${{ secrets.SCRAPE_API_KEY }}
        SCRAPE_PLATFORMS: ${{ github.event.inputs.platforms || 'all' }}
        CLEANUP_THRESHOLD_DAYS: '7'
        RAILWAY_ENVIRONMENT: production
        DISABLE_PUPPETEER: 'true'
        ENABLE_BROWSER_POOL: 'false'
        ENABLE_RATE_LIMITING: 'true'
        SCRAPER_REQUESTS_PER_MINUTE: '12'
        SCRAPER_REQUESTS_PER_HOUR: '360'
      run: |
<<<<<<< HEAD
        if [ "${{ github.event.inputs.cleanup_only }}" = "true" ]; then
          node scripts/schedule-scraping.js cleanup
        else
          node scripts/schedule-scraping.js
=======
        echo "🚀 Starting JobPing scraping..."
        SCRAPERS_INPUT="${{ github.event.inputs.scrapers }}"
        MODE_INPUT="${{ github.event.inputs.mode }}"

        # Default to orchestrator if no specific scrapers requested
        if [ -z "$SCRAPERS_INPUT" ] || [ "$SCRAPERS_INPUT" = "orchestrator" ]; then
          echo "▶️ Running full orchestrator (single cycle)"
          node automation/real-job-runner.cjs --single-run
        else
          # Run individual scrapers via standardized wrappers
          IFS=',' read -ra NAMES <<< "$SCRAPERS_INPUT"
          for s in "${NAMES[@]}"; do
            case "$s" in
              adzuna)
                echo "▶️ Running Adzuna wrapper (CITY=$CITY)"
                if [ -n "$CITY" ]; then 
                  CITY="$CITY" node scrapers/wrappers/adzuna-wrapper.cjs
                else 
                  node scrapers/wrappers/adzuna-wrapper.cjs
                fi
                ;;
              jobspy)
                echo "▶️ Running JobSpy wrapper"
                node scrapers/wrappers/jobspy-wrapper.cjs
                ;;
              reed)
                echo "▶️ Running Reed wrapper"
                node scrapers/wrappers/reed-wrapper.cjs
                ;;
              *)
                echo "❓ Unknown scraper: $s (supported: adzuna, jobspy, reed, orchestrator)"
                ;;
            esac
          done
>>>>>>> f1ca744 (feat(ci): add selectable scraper wrappers and inputs; orchestrator single-run; remove Slack dependency)
        fi

  # Optional: Add a job to run cleanup more frequently
  cleanup:
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' # Only run on schedule, not manual
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        
    - name: Install dependencies
      run: npm ci
      
    - name: Run cleanup
      env:
        SCRAPE_API_URL: ${{ secrets.SCRAPE_API_URL }}
        CLEANUP_API_URL: ${{ secrets.CLEANUP_API_URL }}
        SCRAPE_API_KEY: ${{ secrets.SCRAPE_API_KEY }}
        CLEANUP_THRESHOLD_DAYS: '7'
      run: node scripts/schedule-scraping.js cleanup
