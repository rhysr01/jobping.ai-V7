name: JobPing Automated Scraping

on:
  schedule:
    # 3x daily: 8am, 1pm, 6pm UTC
    - cron: '0 8,13,18 * * *'
  workflow_dispatch:
    inputs:
      scrapers:
        description: 'Comma-separated list: orchestrator, adzuna, jobspy, reed'
        required: false
      mode:
        description: 'prod or test'
        required: false
        default: 'prod'
      city:
        description: 'Optional: limit Adzuna to a single city (e.g. Paris)'
        required: false

concurrency:
  group: scrape-jobping
  cancel-in-progress: true

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
    
    - name: Setup Python for JobSpy
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install python-jobspy
        
    - name: Install Node dependencies
      run: npm ci
      
    - name: Run job scraping
      env:
        # Database
        NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        
        # API Keys (add all your working keys)
        RAPIDAPI_KEY: ${{ secrets.RAPIDAPI_KEY }}
        ADZUNA_APP_ID: ${{ secrets.ADZUNA_APP_ID }}
        ADZUNA_APP_KEY: ${{ secrets.ADZUNA_APP_KEY }}
        REED_API_KEY: ${{ secrets.REED_API_KEY }}
        MUSE_API_KEY: ${{ secrets.MUSE_API_KEY }}
        JOOBLE_API_KEY: ${{ secrets.JOOBLE_API_KEY }}
        GREENHOUSE_API_KEY: ${{ secrets.GREENHOUSE_API_KEY }}
        SERP_API_KEY: ${{ secrets.SERP_API_KEY }}
        
        # OpenAI for job processing
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        
        # Scraping mode
        JOBPING_PRODUCTION_MODE: 'true'
        NODE_ENV: 'production'
        CITY: ${{ github.event.inputs.city }}
      run: |
        echo "üöÄ Starting JobPing scraping..."
        SCRAPERS_INPUT="${{ github.event.inputs.scrapers }}"
        MODE_INPUT="${{ github.event.inputs.mode }}"

        if [ -z "$SCRAPERS_INPUT" ] || [ "$SCRAPERS_INPUT" = "orchestrator" ]; then
          echo "‚ñ∂Ô∏è Running orchestrator (single-run)"
          node automation/real-job-runner.cjs --single-run
        else
          IFS=',' read -ra NAMES <<< "$SCRAPERS_INPUT"
          for s in "${NAMES[@]}"; do
            case "$s" in
              adzuna)
                echo "‚ñ∂Ô∏è Running Adzuna wrapper (CITY=$CITY)"
                if [ -n "$CITY" ]; then CITY="$CITY" node scrapers/wrappers/adzuna-wrapper.cjs; else node scrapers/wrappers/adzuna-wrapper.cjs; fi
                ;;
              jobspy)
                echo "‚ñ∂Ô∏è Running JobSpy wrapper"
                node scrapers/wrappers/jobspy-wrapper.cjs
                ;;
              reed)
                echo "‚ñ∂Ô∏è Running Reed wrapper"
                node scrapers/wrappers/reed-wrapper.cjs
                ;;
              *)
                echo "‚ùì Unknown scraper: $s"
                ;;
            esac
          done
        fi
        
    - name: Verify results
      env:
        NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
        SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
      run: |
        # Quick verification that jobs were saved
        node -e "
        const { createClient } = require('@supabase/supabase-js');
        const supabase = createClient(process.env.NEXT_PUBLIC_SUPABASE_URL, process.env.SUPABASE_SERVICE_ROLE_KEY);
        supabase.from('jobs').select('id', { count: 'exact', head: true }).then(({count, error}) => {
          if (error) throw error;
          console.log('‚úÖ Database health check: ' + count + ' total jobs');
        });
        "
        
    - name: Notify on failure
      if: failure()
      run: |
        echo "‚ùå Job scraping failed - check logs"